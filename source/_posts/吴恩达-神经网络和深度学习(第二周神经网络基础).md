---
title: 吴恩达-神经网络和深度学习(第二周神经网络基础)
date: 2017-09-07 17:50:33
tags: [神经网络, 深度学习]
---

>学习如何用神经网络的思维模式提出机器学习问题、如何使用向量化加速你的模型。

* 先介绍一些名词
  * training set (训练集)
  * feature vector(特征向量)
  * classifier(分类器)
  * calculus（微积分）
  * 循环（loop）
  * 数据集（datasets）
  * vectorization (向量化)
  * matrix(矩阵)
  * vector(向量) 
<!-- more -->
* 本周用到的一些符号【Notation】
  * （x,y）表示一个单独的样本
  * x是xn维的特征向量
  * 标签y值为0/1
  * 训练集由m个训练样本构成
  * (x^(1), y^(1))表示样本一输入和输出，
  * {x^(1), y^(1),.....x^(n), y^(n).}整个训练集
  * 测试集的 样本数。训练集的样本数
  * 矩阵也可表示训练集输入x,输出标签y
  * 注意：**训练样本作为行向量堆叠**
  * 输出y是1xm矩阵

如下图：
![Notation](http://upload-images.jianshu.io/upload_images/4340772-ace3010bd66a3dc3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 二分分类【Binary Classification】
* 神经网络的计算过程中，通常有一个正向过程【forward  pass】（或者正向传播步骤）,接着有一个反向过程【backward pass】(或者反向传播步骤)

* logistic回归是一个用于二分分类的算法
* 计算机保存图片，要保存**三个独立矩阵**（Red红 blue绿 Green蓝）
如果有64X64的一张图片，则**输入向量的维度n=64X64X3=12288**

* 在二分分类问题中，目标是训练出一个**分类器**，他以图片的特征向量x作为输入，，预测输出的结果y(**只有两个值**)


### logistic回归

这是一个学习算法，用在监督学习中，
如下图：
![logistic](http://upload-images.jianshu.io/upload_images/4340772-be2183af3f59d818.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### logistic回归损失函数

损失函数【error function】在单个训练样本中定义的，他衡量了在单个训练样本上的表现
成本函数【cost function 】在全体训练集样本下的表现-->对损失函数求和
对以上函数要找到合适的**参数w和b**
成本函数用于衡量参数w和b的效果

如下图：
![mark](http://upload-images.jianshu.io/upload_images/4340772-d24d5e999f6e5713.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![mark](http://upload-images.jianshu.io/upload_images/4340772-0427fc753f631b58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
### 梯度下降法【Gradient Descent】
* 训练或学习训练集上的参数w和b
![梯度下降法](http://upload-images.jianshu.io/upload_images/4340772-8acc29d69d9d10d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![求最小值](http://upload-images.jianshu.io/upload_images/4340772-fd93c51f67066567.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


### 导数
等于= be equal to 
+  pluse
- minus
X times
/ divide



### 计算图
>可以说，一个神经网络的计算都是按照前向或者反向传播过程来实现的，
首先计算出神经网络的输出
首先计算出神经网络的输出，紧接着一个反向传播操作，后者我们用来计算出对应的梯度或者导数，流程图解释了为什么这样实现
流程图，是用蓝色箭头画出来的的，从左到右的计算

![流程图](http://upload-images.jianshu.io/upload_images/4340772-e97c6467621cc516.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 计算图的导数计算
从右到左计算导数
![mark](http://upload-images.jianshu.io/upload_images/4340772-da5ef2c60f6ae2d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### logistic回归中的梯度下降法（用偏导数实现）

导数流程图来计算梯度用偏导数有点大材小用，但对理解比较好
![mark](http://upload-images.jianshu.io/upload_images/4340772-8eea77bbbe6c0632.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### m个样本的梯度下降
运用到整个样本集中
![for循环](http://upload-images.jianshu.io/upload_images/4340772-a457f3d3e4260106.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
### 向量化
循环很低效率，用向量化来加速运算(np.function)
`z=np.dot(w, x) + b`
![mark](http://upload-images.jianshu.io/upload_images/4340772-e902e7f515075452.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
可能有人说：可扩展深度学习实现是在GPU(图像处理单元)上做的，而我们做的再jupyter notebook上（CPU）
但GPU和CPU都有并行化的指令，SIMD单指令流多数据流，这点对GPU和CPU上面是成立的，只是GPU更擅长SIMD运算

* for循环能不用就不用，如果可以使用内置函数或者其他方法计算循环，会比for循环更快
![mark](http://upload-images.jianshu.io/upload_images/4340772-a95281da70ed860e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![mark](http://upload-images.jianshu.io/upload_images/4340772-2e0b31d42927905e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 进一步向量化logistic回归

![mark](http://upload-images.jianshu.io/upload_images/4340772-a3e45618ebeadbda.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 向量化logistic回归的梯度输出

![mark](http://upload-images.jianshu.io/upload_images/4340772-311ff1bbbbce2c8d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
这就得到高度向量化的，高效的logistic回归梯度下降法

### python中的广播（使python和Numpy部分代码更高效）
广播（broadcasting）对列向量，行向量都有效
例子：
![mark](http://upload-images.jianshu.io/upload_images/4340772-114b9bd2669c3e05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![mark](http://upload-images.jianshu.io/upload_images/4340772-61ed179a1a902989.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

实现神经网络算法时主要用到的广播形式
![mark](http://upload-images.jianshu.io/upload_images/4340772-65f31aa1feadaf7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
### 关于python/numpy向量的说明
千万不要用秩为1的数组
随意插入assert()声明，要仔细检查矩阵和数组的维度
不要害怕调用reshape,来确保你的矩阵和向量

![mark](http://upload-images.jianshu.io/upload_images/4340772-2fa688c563527064.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

可以排除，简化甚至消灭代码中各种奇怪的bug





学习链接
[4. Logistic代码实战](http://www.missshi.cn/api/view/blog/59aa08fee519f50d04000170)