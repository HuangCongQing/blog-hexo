---
title: 吴恩达-神经网络和深度学习（ 第三周 浅层神经网络：）
date: 2017-09-14 17:50:33
tags: [神经网络, 深度学习]
---



>学习使用前向传播和反向传播搭建出有一个隐藏层的神经网络。

* hidden layer Neural Network

###3.1  神经网络概览

![Neutral Network](http://upload-images.jianshu.io/upload_images/4340772-9d7700eb910aea5c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

<!-- more -->
###3.2  神经网络表示
* 双层神经网络（只有一个隐藏层，单隐层神经网络）
* 输入层-四个隐藏层单元-输出层
* 双层神经网络。不把输入层看做一个标准的层 
* 这里的隐藏层有两个相关的参数w和b,使用上标[1]表示这些参数，w是4x3矩阵,b是4x1向量（4代表有四个节点或者隐藏单元，3来自于3个输入特征）
* 输出层也有相关的参数w^[2]  (1x4,隐藏层有四个隐藏单元，输出层只有一个单元),b^[2]【1x1】


![神经网络表示](http://upload-images.jianshu.io/upload_images/4340772-bebf4cc9908dab48.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



###3.3  计算神经网络的输出
>gengrate 生成
compute 计算
matrix 矩阵




![image.png](http://upload-images.jianshu.io/upload_images/4340772-986017821bad5a5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![image.png](http://upload-images.jianshu.io/upload_images/4340772-efce896fa844db88.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

* 神经网络只不过是计算这些步骤很多次


![image.png](http://upload-images.jianshu.io/upload_images/4340772-fd26de856acca219.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![image.png](http://upload-images.jianshu.io/upload_images/4340772-45e07aeec5b0a344.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)





###3.4  多个例子中的向量化
>训练样本横向堆叠构成矩阵X


* m个训练样本
* 用激活函数来表示这些式子`a^[2](i)`【i是训练样本i,2指的是第二层】

![image.png](http://upload-images.jianshu.io/upload_images/4340772-449861dd7dee81f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


* 一个for循环遍历所有的m训练样本
* 向量化


![image.png](http://upload-images.jianshu.io/upload_images/4340772-cc5e1fa3aaa3d854.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


###3.5  向量化实现的解释

![image.png](http://upload-images.jianshu.io/upload_images/4340772-cd2684cca6922347.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这就是对不同训练样本向量化的神经网络，接下来，我们目前为止我们一直都是用sigmoid函数

![image.png](http://upload-images.jianshu.io/upload_images/4340772-4ae6e75a8b9b04ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


###3.6  激活函数（activation function）
>如何使用不同种类的激活函数，simoid只是其中的一个可能选择

* 为了搭建神经网络，可以选择的是选择隐层里用哪个激活函数，还有神经网络的输出单元用什么激活函数
有一个函数总比sigmoid函数表现好，那就是a=tanh(z)（双曲正切函数），
函数介于+1和-1之间，意味着激活函数的平均值更接近0

但 二分分类是，输出层最好用sigmoid,因为y输出介于0-1更合理


![image.png](http://upload-images.jianshu.io/upload_images/4340772-4f9007ef5b15cf2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

不过，sigmoid和tanh函数都有一个缺点，如果z非常大，或非常小，函数的斜率很接近0，这样会拖慢梯度下降算法

修正线性单元（ReLU）就派上用场了（z为正，斜率为1，为负，斜率为0）

![image.png](http://upload-images.jianshu.io/upload_images/4340772-baf970658fb23bc6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在选择激活函数时有一些经验法则
* 如果你的输出值是0或1，如果你在做二元分类，那么sigmoid很适合作为输出层的激活函数，然后其他所有单元都用ReUL。
如果你不确定隐层应该用哪个，那就用ReUL作为激活函数

还有个带泄露的ReUL（z小于0是有一个缓缓的斜率，）通常比ReUL激活函数好，不过实际中使用的频率没那么高

在实践中使用ReUL激活函数，学习速度通常会快得多，比使用tanh或sigmoid激活函数快得多，因为ReUL没有函数斜率接近0时，减慢学习速度的学习速度的效应


说一下几个激活函数
* sigmoid
除非用在二元分类的输出层，不然绝对不要用，或者几乎从来不会用

* 最常用的默认激活函数ReLU，不确定用哪个，就用这个，或者带泄露的ReLU（max(0.01z, z)）

![image.png](http://upload-images.jianshu.io/upload_images/4340772-a12eb94021d55e07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


###3.7  为什么需要非线性激活函数（nonlinear activation function）？

如果用线性激活函数（linear activation function）或者叫做恒等激活函数，那么神经网络只是把输入线性组合再输出，
无论你的神经网络多少层，一直在做的只是计算线性激活函数，和没有任何的标准Logistic回归是一样的，因为两个线性函数组合的本身就是线性函数，除非你引用非线性，

* 只有一个地方可以使用线性激活函数g（z） = z，就是你要机器学习的是回归问题，所以y是一个实数，
* 线性激活函数不可能用在隐藏层（除压缩），要用，也是用在输出层，
![image.png](http://upload-images.jianshu.io/upload_images/4340772-eddcd1e14e9259c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)












###3.8  激活函数的导数

接下来讨论梯度下降的基础，如何估计，如何计算单个激活函数的导数，斜率，



![image.png](http://upload-images.jianshu.io/upload_images/4340772-14b2e0de1b445a63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


![image.png](http://upload-images.jianshu.io/upload_images/4340772-6e9e705bcb8f3e77.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


![image.png](http://upload-images.jianshu.io/upload_images/4340772-90a6b87258f2585e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



![image.png](http://upload-images.jianshu.io/upload_images/4340772-efaa0dcfff5f9aa0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)






###3.9  神经网络的梯度下降法
梯度下降算法的具体实现，如何处理单隐层神经网络，
提供所需的方程，来实现反向传播，或者说梯度下降算法，


keepdims = true(确保python输出的是矩阵)

* 正向传播 4个方程
* 反向传播 6个方程


![image.png](http://upload-images.jianshu.io/upload_images/4340772-a29e026c3008e8cd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




###3.10  （选修）直观理解反向传播


###3.11  随机初始化

当你训练神经网络时，随机初始化权重非常重要，对于logistic回归，可以将权重初始化为零，
但如果将神经网络各参数数组初始化为0，再使用梯度下降算法（gredient descent），那会完全失效

因为我们需要两个不同的隐藏单元，去计算不同的函数，**这个问题的解决方案是随机初始化所有参数，**
* 可以令W^[1] = np.random,randn, 这可以产生参数为（2， 2）的高斯分布随机变量，然后你再乘以一个很小的数字【因为通常喜欢把权重矩阵初始化非常小的随机数】，所有你将权重初始化很小的随机数，
* 如果训练单隐层神经网络时，没有太多的隐藏层，设为0.01还可以，但当训练一个很深的神经网络时，可能要试试0.01以外的常数，
* 把b初始化0是可以的



![随机初始化](http://upload-images.jianshu.io/upload_images/4340772-4f5e7d7d5861f341.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




>所以，在这周的视频里，你知道如何设立单隐层神经网络，初始化参数，并用正向传播计算预测值，还有计算导数，然后使用梯度下降，反向传播
